import streamlit as st
from ui.auth import auth
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import base64
import random
import os
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from utils import (
    load_sample_data, get_or_train_model, display_model_metrics, predict_sentiment,
    preprocess_text, get_word_frequencies, get_ngrams, create_wordcloud, get_table_download_link
)

def render_dashboard():
    # Sinkronisasi status login dari cookie ke session_state (penting untuk refresh)
    auth.sync_login_state()
    # Tampilkan toast jika login baru saja berhasil (untuk fallback jika main.py tidak sempat menampilkan)
    if st.session_state.get('login_success', False):
        st.toast(f"User {st.session_state.get('user_email', '')} login successfully!", icon="âœ…")
        st.session_state['login_success'] = False
    # Load data dan model (cache)
    data = load_sample_data()
    if 'data_loaded_toast_shown' not in st.session_state:
        if not data.empty:
            st.toast(f"Data berhasil dimuat: {len(data)} ulasan", icon="âœ…")
        else:
            st.toast("Data gagal dimuat atau kosong!", icon="âš ï¸")
        st.session_state['data_loaded_toast_shown'] = True
    preprocessing_options = {
        'lowercase': True,
        'clean_text': True,
        'normalize_slang': True,
        'remove_repeated': True,
        'remove_punctuation': True,
        'remove_numbers': True,
        'tokenize': True,
        'remove_stopwords': True,
        'stemming': True,
        'rejoin': True
    }
    pipeline, accuracy, precision, recall, f1, confusion_mat, X_test, y_test, tfidf_vectorizer, svm_model = get_or_train_model(data, preprocessing_options)    # =============== HEADER DAN FILTER SECTION YANG DIPERBAIKI ===============
    st.title("ðŸ“Š Dashboard Analisis Sentimen GoRide")
    st.markdown("**Analisis Komprehensif Ulasan Pengguna dari Google Play Store**")
    
    # Info banner dengan model performance
    with st.container():
        col1, col2, col3, col4, col5 = st.columns(5)
        with col1:
            st.metric("Model Accuracy", f"{accuracy:.1%}")
        with col2:
            st.metric("Precision", f"{precision:.1%}")
        with col3:
            st.metric("Recall", f"{recall:.1%}")
        with col4:
            st.metric("F1-Score", f"{f1:.1%}")
        with col5:
            total_data_count = len(data)
            st.metric("Total Dataset", f"{total_data_count:,}")
    
    st.markdown("---")
    
    # Filter Section yang diperbaiki
    with st.expander("ðŸŽ›ï¸ **Pengaturan Filter & Konfigurasi**", expanded=True):
        st.markdown("### ðŸ“… **Filter Berdasarkan Rentang Waktu**")
        col1, col2, col3 = st.columns([2, 2, 1])
        with col1:
            start_date = st.date_input("ðŸ“… Tanggal Mulai", value=pd.to_datetime(data['date']).min())
        with col2:
            end_date = st.date_input("ðŸ“… Tanggal Selesai", value=pd.to_datetime(data['date']).max())
        with col3:
            if st.button("ðŸ”„ Reset Filter"):
                st.rerun()
        
        # Quick filter presets
        st.markdown("**âš¡ Filter Cepat:**")
        quick_filter_col1, quick_filter_col2, quick_filter_col3, quick_filter_col4 = st.columns(4)
        
        with quick_filter_col1:
            if st.button("ðŸ“… 7 Hari Terakhir"):
                end_date = pd.to_datetime(data['date']).max()
                start_date = end_date - pd.Timedelta(days=7)
        with quick_filter_col2:
            if st.button("ðŸ“… 30 Hari Terakhir"):
                end_date = pd.to_datetime(data['date']).max()
                start_date = end_date - pd.Timedelta(days=30)
        with quick_filter_col3:
            if st.button("ðŸ“… 3 Bulan Terakhir"):
                end_date = pd.to_datetime(data['date']).max()
                start_date = end_date - pd.Timedelta(days=90)
        with quick_filter_col4:
            if st.button("ðŸ“… Tahun Ini"):
                end_date = pd.to_datetime(data['date']).max()
                start_date = pd.Timestamp(end_date.year, 1, 1)    
    start_date = pd.to_datetime(start_date)
    end_date = pd.to_datetime(end_date)
    
    # Progress bar untuk loading
    progress_bar = st.progress(0)
    with st.spinner('ðŸ”„ Memproses dan memfilter data...'):
        progress_bar.progress(25)
        filtered_data = data[(pd.to_datetime(data['date']) >= start_date) & (pd.to_datetime(data['date']) <= end_date)]
        progress_bar.progress(75)
        
    progress_bar.progress(100)
    progress_bar.empty()
    
    if filtered_data.empty:
        st.warning("âš ï¸ Tidak ada data yang sesuai dengan filter yang dipilih. Silakan ubah rentang tanggal.")
        st.info("ðŸ’¡ **Tip:** Coba perluas rentang tanggal atau gunakan filter cepat yang tersedia.")
        return
    
    # Display info tentang data yang difilter
    date_range_info = f"ðŸ“Š Menampilkan **{len(filtered_data):,}** ulasan dari **{start_date.strftime('%d %B %Y')}** hingga **{end_date.strftime('%d %B %Y')}**"
    st.info(date_range_info)
    
    @st.cache_data(ttl=300)
    def calculate_metrics(df):
        total = len(df)
        pos_count = len(df[df['sentiment'] == 'POSITIF'])
        neg_count = len(df[df['sentiment'] == 'NEGATIF'])
        pos_percentage = (pos_count / total * 100) if total > 0 else 0
        neg_percentage = (neg_count / total * 100) if total > 0 else 0
        pos_pct = pos_count/total*100 if total > 0 else 0
        today = pd.Timestamp.now().strftime('%Y-%m-%d')
        today_count = len(df[df['date'] == today])
        
        # Additional metrics
        avg_daily = total / max(1, (pd.to_datetime(df['date']).max() - pd.to_datetime(df['date']).min()).days + 1)
        
        return {
            'total': total,
            'pos_count': pos_count,
            'neg_count': neg_count,
            'pos_percentage': pos_percentage,
            'neg_percentage': neg_percentage,
            'today_count': today_count,
            'pos_pct': pos_pct,
            'avg_daily': avg_daily
        }
    
    metrics = calculate_metrics(filtered_data)
    
    # =============== METRICS DISPLAY YANG DIPERBAIKI ===============
    st.markdown("### ðŸ“ˆ **Key Performance Indicators**")
    
    # Baris pertama - Metrics utama
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric(
            label="ðŸ“Š Total Ulasan", 
            value=f"{metrics['total']:,}", 
            delta=f"{metrics['today_count']} hari ini",
            help="Total jumlah ulasan dalam periode yang dipilih"
        )
    with col2:
        delta_pos = f"{metrics['pos_percentage'] - 50:+.1f}% dari rata-rata"
        st.metric(
            label="ðŸŸ¢ Sentimen Positif", 
            value=f"{metrics['pos_percentage']:.1f}%", 
            delta=delta_pos,
            help="Persentase ulasan dengan sentimen positif"
        )
    with col3:
        delta_neg = f"{metrics['neg_percentage'] - 50:+.1f}% dari rata-rata"
        st.metric(
            label="ðŸ”´ Sentimen Negatif", 
            value=f"{metrics['neg_percentage']:.1f}%", 
            delta=delta_neg,
            delta_color="inverse",
            help="Persentase ulasan dengan sentimen negatif"
        )
    with col4:
        satisfaction_score = metrics['pos_pct']
        if satisfaction_score >= 80:
            satisfaction_label = "Excellent ðŸŒŸ"
        elif satisfaction_score >= 60:
            satisfaction_label = "Good âœ…"
        elif satisfaction_score >= 40:
            satisfaction_label = "Fair âš ï¸"
        else:
            satisfaction_label = "Poor âŒ"
            
        st.metric(
            label="ðŸ‘ Indeks Kepuasan", 
            value=f"{satisfaction_score:.1f}%",
            delta=satisfaction_label,
            help="Indeks kepuasan berdasarkan rasio sentimen positif"
        )
    
    # Baris kedua - Metrics tambahan
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric(
            label="ðŸ“Š Rata-rata Harian", 
            value=f"{metrics['avg_daily']:.1f}",
            help="Rata-rata jumlah ulasan per hari dalam periode ini"
        )
    with col2:
        period_days = (end_date - start_date).days + 1
        st.metric(
            label="ðŸ“… Periode Analisis", 
            value=f"{period_days} hari",
            help="Rentang waktu yang dianalisis"
        )
    with col3:
        data_coverage = (len(filtered_data) / len(data) * 100)
        st.metric(
            label="ðŸ“‹ Coverage Data", 
            value=f"{data_coverage:.1f}%",
            help="Persentase data yang tercakup dalam filter"
        )
    with col4:
        if metrics['total'] > 0:
            confidence_level = "Tinggi" if metrics['total'] >= 100 else "Sedang" if metrics['total'] >= 30 else "Rendah"
            st.metric(
                label="ðŸŽ¯ Confidence Level", 
                value=confidence_level,
                help="Tingkat kepercayaan analisis berdasarkan jumlah sampel"
            )    # =============== PREPROCESSING DAN TOPIC FILTER YANG DIPERBAIKI ===============
    # Pastikan kolom teks_preprocessing tersedia dan konsisten
    if 'teks_preprocessing' not in data.columns:
        with st.spinner("ðŸ”„ Melakukan preprocessing batch untuk seluruh data..."):
            data.loc[:, 'teks_preprocessing'] = data['review_text'].astype(str).apply(lambda x: preprocess_text(x, preprocessing_options))
    
    if 'teks_preprocessing' not in filtered_data.columns:
        filtered_data = filtered_data.copy()
        with st.spinner("âš™ï¸ Preprocessing data yang difilter..."):
            filtered_data.loc[:, 'teks_preprocessing'] = filtered_data['review_text'].astype(str).apply(lambda x: preprocess_text(x, preprocessing_options))
    
    st.markdown("---")
    
    # Topic Filter Section yang diperbaiki
    with st.expander("ðŸ·ï¸ **Filter Berdasarkan Topik/Kata Kunci**", expanded=False):
        st.markdown("### ðŸ” **Analisis Topik Trending**")
        
        # Ambil topik dari top 20 kata paling sering muncul di hasil preprocessing
        all_words = " ".join(filtered_data['teks_preprocessing'])
        word_freq = get_word_frequencies(all_words, top_n=25)
        
        # Tampilkan word frequency dalam bentuk yang lebih menarik
        if word_freq:
            st.markdown("**ðŸ“Š Top Keywords dalam Periode Ini:**")
            
            # Display top keywords dengan visual yang lebih baik
            col1, col2, col3 = st.columns(3)
            top_words = list(word_freq.items())
            
            for i, (word, freq) in enumerate(top_words[:15]):
                col_idx = i % 3
                if col_idx == 0:
                    with col1:
                        st.write(f"**{i+1}.** {word} `({freq}x)`")
                elif col_idx == 1:
                    with col2:
                        st.write(f"**{i+1}.** {word} `({freq}x)`")
                else:
                    with col3:
                        st.write(f"**{i+1}.** {word} `({freq}x)`")
        
        # Topic selection dengan kategori yang lebih user-friendly
        topics = ["ðŸŒ Semua Topik"] + [f"ðŸ”‘ {word}" for word in word_freq.keys()]
        
        col1, col2 = st.columns([3, 1])
        with col1:
            selected_topic_display = st.selectbox(
                "ðŸŽ¯ Pilih Topik untuk Analisis Mendalam:", 
                topics,
                help="Pilih topik spesifik untuk melihat analisis yang lebih fokus"
            )
        with col2:
            if st.button("ðŸ”„ Reset Topik"):
                selected_topic_display = "ðŸŒ Semua Topik"
        
        # Extract actual topic name (remove emoji prefix)
        if selected_topic_display == "ðŸŒ Semua Topik":
            selected_topic = "All"
        else:
            selected_topic = selected_topic_display.replace("ðŸ”‘ ", "")
    
    # Filter data berdasarkan topik yang dipilih
    if selected_topic != "All":
        topic_data = filtered_data[filtered_data['teks_preprocessing'].str.contains(selected_topic, case=False, na=False)].copy()
        
        # Informasi hasil filter dengan styling yang lebih baik
        if len(topic_data) > 0:
            coverage_pct = (len(topic_data) / len(filtered_data) * 100)
            st.success(f"ðŸŽ¯ **Filter Aktif:** Ditemukan **{len(topic_data):,}** ulasan untuk topik **'{selected_topic}'** ({coverage_pct:.1f}% dari total data)")
            
            # Mini metrics untuk topik spesifik
            topic_metrics = calculate_metrics(topic_data)
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Ulasan Topik", f"{len(topic_data):,}")
            with col2:
                st.metric("% Positif", f"{topic_metrics['pos_percentage']:.1f}%")
            with col3:
                st.metric("% Negatif", f"{topic_metrics['neg_percentage']:.1f}%")
        else:
            st.warning(f"âš ï¸ Tidak ada data untuk topik **'{selected_topic}'**. Kemungkinan kata telah difilter dalam preprocessing atau stemming.")
            st.info("ðŸ’¡ **Tip:** Coba pilih topik lain atau gunakan 'Semua Topik' untuk melihat keseluruhan data.")
            topic_data = filtered_data.copy()
    else:
        topic_data = filtered_data.copy()
        st.info("ðŸŒ **Menampilkan semua topik** - Gunakan filter topik di atas untuk analisis yang lebih spesifik")
    
    # Ensure topic_data has the preprocessing column
    if 'teks_preprocessing' not in topic_data.columns:
        topic_data = topic_data.copy()
        topic_data.loc[:, 'teks_preprocessing'] = topic_data['review_text'].astype(str).apply(lambda x: preprocess_text(x, preprocessing_options))
    
    # Fallback jika masih kosong
    if topic_data.empty:
        st.error("âŒ Data kosong setelah filtering. Menggunakan data lengkap sebagai fallback.")
        topic_data = filtered_data.copy()
    tab1, tab2, tab3 = st.tabs(["Distribusi Sentimen", "Tren Waktu", "Analisis Kata"])
    with tab1:
        st.subheader("ðŸ“Š Distribusi Sentimen")
        col1, col2 = st.columns(2)
        with col1:
            sentiment_counts = topic_data['sentiment'].value_counts().reset_index()
            sentiment_counts.columns = ['Sentiment', 'Count']
            bar_chart = px.bar(sentiment_counts, x='Sentiment', y='Count', color='Sentiment', color_discrete_map={'POSITIF': 'green', 'NEGATIF': 'red'}, title="Distribusi Sentimen Ulasan")
            st.plotly_chart(bar_chart, use_container_width=True)
        with col2:
            pie_chart = px.pie(sentiment_counts, values='Count', names='Sentiment', color='Sentiment', color_discrete_map={'POSITIF': 'green', 'NEGATIF': 'red'}, title="Persentase Sentimen Ulasan")
            pie_chart.update_traces(textposition='inside', textinfo='percent+label')
            st.plotly_chart(pie_chart, use_container_width=True)
    with tab2:
        st.subheader("ðŸ“ˆ Tren Sentimen dari Waktu ke Waktu")
        time_granularity = st.radio("Granularitas Waktu:", options=["Harian", "Mingguan", "Bulanan"], horizontal=True)
        visualization_data = topic_data
        if len(topic_data) > 10000:
            sample_size = min(10000, int(len(topic_data) * 0.3))
            st.info(f"Dataset terlalu besar untuk visualisasi ({len(topic_data):,} baris). Menggunakan sampel {sample_size:,} baris untuk memperlancar visualisasi.")
            visualization_data = topic_data.sample(sample_size, random_state=42)
            use_all_data = st.checkbox("Gunakan semua data (bisa memperlambat aplikasi)")
            if use_all_data:
                visualization_data = topic_data
                st.warning("Menggunakan semua data dapat memperlambat visualisasi. Harap bersabar.")
        if time_granularity == "Harian":
            visualization_data['time_group'] = pd.to_datetime(visualization_data['date']).dt.strftime('%Y-%m-%d')
            unique_days = visualization_data['time_group'].nunique()
            if unique_days > 100:
                st.info(f"Data harian terlalu banyak ({unique_days} hari). Menerapkan binning otomatis.")
                visualization_data['time_group'] = pd.to_datetime(visualization_data['date']).dt.to_period('W').astype(str)
        elif time_granularity == "Mingguan":
            visualization_data['time_group'] = pd.to_datetime(visualization_data['date']).dt.strftime('%Y-%W')
        else:
            visualization_data['time_group'] = pd.to_datetime(visualization_data['date']).dt.strftime('%Y-%m')
        sentiment_trend = topic_data.groupby(['time_group', 'sentiment']).size().reset_index(name='count')
        try:
            sentiment_pivot = sentiment_trend.pivot(index='time_group', columns='sentiment', values='count').reset_index()
            sentiment_pivot.fillna(0, inplace=True)
            if 'POSITIF' not in sentiment_pivot.columns:
                sentiment_pivot['POSITIF'] = 0
            if 'NEGATIF' not in sentiment_pivot.columns:
                sentiment_pivot['NEGATIF'] = 0
            sentiment_pivot['total'] = sentiment_pivot['POSITIF'] + sentiment_pivot['NEGATIF']
            sentiment_pivot['positive_percentage'] = np.where(sentiment_pivot['total'] > 0, (sentiment_pivot['POSITIF'] / sentiment_pivot['total'] * 100).round(2), 0)
            line_chart = px.line(sentiment_pivot, x='time_group', y=['POSITIF', 'NEGATIF'], title=f"Tren Jumlah Ulasan Berdasarkan Sentimen ({time_granularity})", labels={'value': 'Jumlah Ulasan', 'time_group': 'Waktu', 'variable': 'Sentimen'}, color_discrete_map={'POSITIF': 'green', 'NEGATIF': 'red'})
            st.plotly_chart(line_chart, use_container_width=True)
            pct_line_chart = px.line(sentiment_pivot, x='time_group', y='positive_percentage', title=f"Tren Persentase Sentimen Positif ({time_granularity})", labels={'positive_percentage': '% Ulasan Positif', 'time_group': 'Waktu'})
            pct_line_chart.update_traces(line_color='green')
            pct_line_chart.add_shape(type="line", x0=0, y0=50, x1=1, y1=50, xref="paper", line=dict(color="gray", width=1, dash="dash"))
            st.plotly_chart(pct_line_chart, use_container_width=True)
            csv = sentiment_pivot.to_csv(index=False)
            b64 = base64.b64encode(csv.encode()).decode()
            href = f'<a href="data:file/csv;base64,{b64}" download="sentiment_trend.csv">ðŸ“¥ Download Trend Data (CSV)</a>'
            st.markdown(href, unsafe_allow_html=True)
        except Exception as e:
            st.error(f"Error generating trend charts: {e}")
            st.info("Try adjusting your date range or filters to include more data points.")
    with tab3:
        st.subheader("ðŸ“ Analisis Kata dalam Ulasan")
        col1, col2 = st.columns(2)
        @st.cache_data(ttl=3600)
        def safe_create_wordcloud(text, max_words=100, max_length=10000, timeout_seconds=15):
            import threading
            import signal
            import time
            class TimeoutException(Exception):
                pass
            def timeout_handler(signum, frame):
                raise TimeoutException("Wordcloud generation timed out")
            result = [None]
            if len(text) > max_length:
                st.info(f"Text size reduced from {len(text):,} to {max_length:,} characters for efficient wordcloud generation")
                words = text.split()
                sampled_words = random.sample(words, min(max_length, len(words)))
                text = " ".join(sampled_words)
            reduce_complexity = False
            current_memory = 0
            try:
                import psutil
                process = psutil.Process(os.getpid())
                current_memory = process.memory_info().rss / 1024 / 1024
                if current_memory > 1000:
                    reduce_complexity = True
            except Exception:
                pass
            if reduce_complexity or len(text) > 100000:
                max_words = min(50, max_words)
                st.info(f"Reducing wordcloud complexity due to high memory usage or large text size.")
            try:
                if hasattr(signal, 'SIGALRM'):
                    signal.signal(signal.SIGALRM, timeout_handler)
                    signal.alarm(timeout_seconds)
                    start_time = time.time()
                    wordcloud = create_wordcloud(text, max_words=max_words)
                    generation_time = time.time() - start_time
                    signal.alarm(0)
                else:
                    import threading
                    result = [None]
                    error = [None]
                    def target_func():
                        try:
                            result[0] = create_wordcloud(text, max_words=max_words)
                        except Exception as e:
                            error[0] = str(e)
                    thread = threading.Thread(target=target_func)
                    start_time = time.time()
                    thread.start()
                    thread.join(timeout_seconds)
                    generation_time = time.time() - start_time
                    if thread.is_alive():
                        st.warning(f"Wordcloud generation timed out after {timeout_seconds} seconds. Trying with smaller sample...")
                        words = text.split()
                        smaller_sample = random.sample(words, min(1000, len(words)))
                        sampled_text = " ".join(smaller_sample)
                        return create_wordcloud(sampled_text, max_words=50)
                    wordcloud = result[0]
                    if error[0] is not None:
                        raise Exception(error[0])
                if generation_time > 3:
                    st.info(f"Wordcloud generated in {generation_time:.1f} seconds")
                return wordcloud
            except TimeoutException:
                st.warning(f"Wordcloud generation timed out after {timeout_seconds} seconds. Trying with smaller sample...")
                words = text.split()
                smaller_sample = random.sample(words, min(1000, len(words)))
                sampled_text = " ".join(smaller_sample)
                return create_wordcloud(sampled_text, max_words=50)
            except Exception as e:
                st.error(f"Error generating wordcloud: {str(e)}")
                return None
        with col1:
            st.write("### ðŸŸ¢ Wordcloud Ulasan Positif")
            positive_reviews = topic_data[topic_data['sentiment'] == 'POSITIF']
            if not positive_reviews.empty:
                positive_text = " ".join(positive_reviews['teks_preprocessing'])
                if positive_text.strip():
                    with st.spinner('Generating positive word cloud...'):
                        pos_wordcloud = safe_create_wordcloud(positive_text)
                        if pos_wordcloud is not None:
                            import matplotlib.pyplot as plt
                            fig, ax = plt.subplots(figsize=(10, 5))
                            ax.imshow(pos_wordcloud, interpolation='bilinear')
                            ax.axis('off')
                            st.pyplot(fig, use_container_width=True)
            st.write("#### Top Kata Positif berdasarkan TF-IDF")
            feature_names = tfidf_vectorizer.get_feature_names_out()
            pos_samples = positive_reviews['teks_preprocessing']
            pos_tfidf = tfidf_vectorizer.transform(pos_samples)
            if pos_tfidf.shape[0] > 0:
                pos_importance = np.asarray(pos_tfidf.mean(axis=0)).flatten()
                pos_indices = np.argsort(pos_importance)[-10:]
                pos_words_df = pd.DataFrame({'Word': [feature_names[i] for i in pos_indices], 'Importance': [pos_importance[i] for i in pos_indices]})
                fig = px.bar(pos_words_df, x='Importance', y='Word', orientation='h', title="Kata Kunci dalam Ulasan Positif", color='Importance', color_continuous_scale='Greens')
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("Tidak ada data ulasan positif untuk ditampilkan.")
        with col2:
            st.write("### ðŸ”´ Wordcloud Ulasan Negatif")
            negative_reviews = topic_data[topic_data['sentiment'] == 'NEGATIF']
            if not negative_reviews.empty:
                negative_text = " ".join(negative_reviews['teks_preprocessing'])
                if negative_text.strip():
                    with st.spinner('Generating negative word cloud...'):
                        neg_wordcloud = safe_create_wordcloud(negative_text)
                        if neg_wordcloud is not None:
                            import matplotlib.pyplot as plt
                            fig, ax = plt.subplots(figsize=(10, 5))
                            ax.imshow(neg_wordcloud, interpolation='bilinear')
                            ax.axis('off')
                            st.pyplot(fig, use_container_width=True)
            st.write("#### Top Kata Negatif berdasarkan TF-IDF")
            feature_names = tfidf_vectorizer.get_feature_names_out()
            neg_samples = negative_reviews['teks_preprocessing']
            neg_tfidf = tfidf_vectorizer.transform(neg_samples)
            if neg_tfidf.shape[0] > 0:
                neg_importance = np.asarray(neg_tfidf.mean(axis=0)).flatten()
                neg_indices = np.argsort(neg_importance)[-10:]
                neg_words_df = pd.DataFrame({'Word': [feature_names[i] for i in neg_indices], 'Importance': [neg_importance[i] for i in neg_indices]})
                fig = px.bar(neg_words_df, x='Importance', y='Word', orientation='h', title="Kata Kunci dalam Ulasan Negatif", color='Importance', color_continuous_scale='Reds')
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("Tidak ada data ulasan negatif untuk ditampilkan.")
        st.subheader("ðŸ” Analisis Topik")
        bigrams = get_ngrams(" ".join(topic_data['teks_preprocessing']), 2, top_n=20)
        bigrams_df = pd.DataFrame(list(bigrams.items()), columns=['Bigram', 'Frequency'])
        fig = px.bar(bigrams_df.sort_values('Frequency', ascending=True).tail(10), x='Frequency', y='Bigram', orientation='h', title="Top 10 Frasa yang Paling Sering Muncul", color='Frequency', color_continuous_scale='Viridis')
        st.plotly_chart(fig, use_container_width=True)    # =============== TABEL ULASAN INTERAKTIF YANG DIPERBAIKI ===============
    st.markdown("---")
    st.header("ðŸ“‹ Eksplorasi Data Ulasan")
    
    with st.expander("ðŸ” **Filter & Pencarian Data**", expanded=True):
        # Search and filter controls
        col1, col2, col3 = st.columns(3)
        
        with col1:
            search_term = st.text_input(
                "ðŸ” Cari dalam Ulasan:", 
                "", 
                placeholder="Ketik kata kunci untuk mencari...",
                help="Pencarian akan dilakukan pada teks yang telah diproses"
            )
        
        with col2:
            sentiment_filter = st.multiselect(
                "ðŸŽ­ Filter Sentimen:", 
                options=["POSITIF", "NEGATIF"], 
                default=["POSITIF", "NEGATIF"],
                help="Pilih jenis sentimen yang ingin ditampilkan"
            )
        
        with col3:
            sort_option = st.selectbox(
                "ðŸ“Š Urutkan berdasarkan:", 
                ["Terbaru", "Terlama", "Sentimen (Positif Dulu)", "Sentimen (Negatif Dulu)"],
                help="Pilih cara pengurutan data"
            )
        
        # Advanced options
        col1, col2, col3 = st.columns(3)
        
        with col1:
            show_confidence = st.checkbox(
                "ðŸŽ¯ Tampilkan Confidence Score", 
                help="Menampilkan tingkat kepercayaan model untuk setiap prediksi"
            )
        
        with col2:
            rows_per_page = st.slider(
                "ðŸ“„ Baris per halaman:", 
                min_value=10, 
                max_value=100, 
                value=25, 
                step=5,
                help="Jumlah ulasan yang ditampilkan per halaman"
            )
        
        with col3:
            compact_view = st.checkbox(
                "ðŸ“± Tampilan Ringkas", 
                help="Menampilkan data dalam format yang lebih ringkas"
            )
    
    # Apply filters
    filtered_display = topic_data.copy()
    
    if search_term:
        search_mask = filtered_display['teks_preprocessing'].str.contains(search_term, case=False, na=False)
        filtered_display = filtered_display[search_mask]
        if len(filtered_display) == 0:
            st.warning(f"ðŸ” Tidak ditemukan ulasan yang mengandung kata **'{search_term}'**")
            filtered_display = topic_data.copy()
        else:
            st.info(f"ðŸ” Ditemukan **{len(filtered_display):,}** ulasan yang mengandung **'{search_term}'**")
    
    if sentiment_filter:
        filtered_display = filtered_display[filtered_display['sentiment'].isin(sentiment_filter)]
    
    # Apply sorting
    if sort_option == "Terbaru":
        filtered_display = filtered_display.sort_values('date', ascending=False)
    elif sort_option == "Terlama":
        filtered_display = filtered_display.sort_values('date', ascending=True)
    elif sort_option == "Sentimen (Positif Dulu)":
        filtered_display = filtered_display.sort_values('sentiment', ascending=False)
    elif sort_option == "Sentimen (Negatif Dulu)":
        filtered_display = filtered_display.sort_values('sentiment', ascending=True)
    
    # Pagination
    total_rows = len(filtered_display)
    total_pages = max(1, total_rows // rows_per_page + (1 if total_rows % rows_per_page > 0 else 0))
    
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        current_page = st.number_input(
            "ðŸ“„ Halaman:", 
            min_value=1, 
            max_value=total_pages, 
            value=1, 
            step=1,
            help=f"Halaman 1 dari {total_pages} (Total: {total_rows:,} ulasan)"
        )
    
    start_idx = (current_page - 1) * rows_per_page
    end_idx = min(start_idx + rows_per_page, total_rows)
    paginated_data = filtered_display.iloc[start_idx:end_idx].copy()
    
    # Display data info
    st.info(f"ðŸ“Š Menampilkan ulasan **{start_idx+1}** - **{end_idx}** dari **{total_rows:,}** ulasan yang difilter (Halaman **{current_page}** dari **{total_pages}**)")
    
    # Prepare data for display
    if show_confidence and not paginated_data.empty:
        with st.spinner("ðŸ§  Menghitung confidence score..."):
            paginated_data['confidence'] = paginated_data['review_text'].apply(
                lambda x: predict_sentiment(x, pipeline)['confidence']
            )
    
    # Convert data types for display
    display_data = paginated_data.copy()
    for col in display_data.columns:
        if display_data[col].dtype == 'object':
            display_data[col] = display_data[col].astype(str)
        elif pd.api.types.is_numeric_dtype(display_data[col]):
            display_data[col] = display_data[col].astype(str)
    
    # Display the table with styling
    if not display_data.empty:
        if compact_view:
            # Compact view - only show essential columns
            essential_cols = ['date', 'sentiment', 'review_text']
            if show_confidence and 'confidence' in display_data.columns:
                essential_cols.append('confidence')
            
            display_data_compact = display_data[essential_cols].copy()
            
            # Shorten review text for compact view
            if 'review_text' in display_data_compact.columns:
                display_data_compact['review_text'] = display_data_compact['review_text'].str[:100] + "..."
            
            st.dataframe(
                display_data_compact,
                height=400,
                use_container_width=True,
                column_config={
                    "date": st.column_config.DateColumn("ðŸ“… Tanggal"),
                    "sentiment": st.column_config.TextColumn("ðŸŽ­ Sentimen"),
                    "review_text": st.column_config.TextColumn("ðŸ’¬ Ulasan (Ringkas)"),
                    "confidence": st.column_config.NumberColumn("ðŸŽ¯ Confidence", format="%.2f") if show_confidence else None
                }
            )
        else:
            # Full view
            st.dataframe(
                display_data,
                height=400,
                use_container_width=True,
                column_config={
                    "date": st.column_config.DateColumn("ðŸ“… Tanggal"),
                    "sentiment": st.column_config.TextColumn("ðŸŽ­ Sentimen"),
                    "review_text": st.column_config.TextColumn("ðŸ’¬ Ulasan Lengkap"),
                    "teks_preprocessing": st.column_config.TextColumn("âš™ï¸ Teks Terproses"),
                    "confidence": st.column_config.NumberColumn("ðŸŽ¯ Confidence", format="%.2f") if show_confidence else None
                }
            )
    else:
        st.warning("âš ï¸ Tidak ada data yang sesuai dengan filter yang dipilih.")
    
    # Download section
    col1, col2, col3 = st.columns(3)
    with col2:
        if not filtered_display.empty:
            st.markdown(
                get_table_download_link(filtered_display, "goride_reviews_filtered", "ðŸ“¥ Download Data Terfilter (CSV)"), 
                unsafe_allow_html=True
            )
        else:
            st.info("Tidak ada data untuk diunduh")
    
    st.markdown("---")# =============== BAGIAN RINGKASAN INSIGHTS YANG DIPERBAIKI ===============
    st.markdown("---")
    st.header("ðŸ’¡ Ringkasan Analisis & Insights")
    
    # Kalkulasi metrics yang diperbaiki
    pos_pct = metrics['pos_percentage']
    neg_pct = metrics['neg_percentage']
    total_reviews = metrics['total']
    
    # Ambil data kata-kata untuk insight yang lebih baik
    pos_reviews = filtered_display[filtered_display['sentiment'] == 'POSITIF']
    neg_reviews = filtered_display[filtered_display['sentiment'] == 'NEGATIF']
    
    if not pos_reviews.empty:
        pos_terms = get_word_frequencies(" ".join(pos_reviews['teks_preprocessing']), top_n=10)
    else:
        pos_terms = {}
        
    if not neg_reviews.empty:
        neg_terms = get_word_frequencies(" ".join(neg_reviews['teks_preprocessing']), top_n=10)
    else:
        neg_terms = {}
    
    # Status Sentiment dengan Visual Indicator yang lebih baik
    col1, col2, col3 = st.columns([1, 2, 1])
    
    with col2:
        if pos_pct >= 80:
            st.success("ðŸŽ‰ **STATUS: SANGAT POSITIF**")
            sentiment_status = "Excellent"
            status_color = "success"
        elif pos_pct >= 60:
            st.success("âœ… **STATUS: POSITIF**")
            sentiment_status = "Good"
            status_color = "success"
        elif pos_pct >= 40:
            st.warning("âš ï¸ **STATUS: NETRAL/CAMPURAN**")
            sentiment_status = "Mixed"
            status_color = "warning"
        else:
            st.error("âŒ **STATUS: NEGATIF**")
            sentiment_status = "Needs Attention"
            status_color = "error"
    
    # Detail Insights dalam Expander untuk organisasi yang lebih baik
    with st.expander("ðŸ“Š **Analisis Detail Sentimen**", expanded=True):
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### ðŸŸ¢ **Sentimen Positif**")
            if pos_pct > 0:
                st.metric("Persentase", f"{pos_pct:.1f}%", delta=f"{pos_pct-50:.1f}% dari rata-rata")
                st.write(f"**Jumlah Ulasan:** {len(pos_reviews):,} dari {total_reviews:,}")
                
                if pos_terms:
                    st.write("**Kata Kunci Dominan:**")
                    for i, (word, freq) in enumerate(list(pos_terms.items())[:5], 1):
                        st.write(f"{i}. **{word}** ({freq} kali)")
                else:
                    st.info("Tidak ada kata kunci positif ditemukan")
            else:
                st.info("Tidak ada ulasan positif dalam periode ini")
        
        with col2:
            st.markdown("#### ðŸ”´ **Sentimen Negatif**")
            if neg_pct > 0:
                st.metric("Persentase", f"{neg_pct:.1f}%", delta=f"{neg_pct-50:.1f}% dari rata-rata", delta_color="inverse")
                st.write(f"**Jumlah Ulasan:** {len(neg_reviews):,} dari {total_reviews:,}")
                
                if neg_terms:
                    st.write("**Masalah Utama:**")
                    for i, (word, freq) in enumerate(list(neg_terms.items())[:5], 1):
                        st.write(f"{i}. **{word}** ({freq} kali)")
                else:
                    st.info("Tidak ada kata kunci negatif ditemukan")
            else:
                st.info("Tidak ada ulasan negatif dalam periode ini")
    
    # Analisis Tren (jika tersedia)
    trend_insights = []
    if 'sentiment_pivot' in locals() and len(sentiment_pivot) > 1:
        first_ratio = sentiment_pivot.iloc[0]['positive_percentage'] if 'positive_percentage' in sentiment_pivot.columns else 0
        last_ratio = sentiment_pivot.iloc[-1]['positive_percentage'] if 'positive_percentage' in sentiment_pivot.columns else 0
        trend_change = last_ratio - first_ratio
        
        with st.expander("ðŸ“ˆ **Analisis Tren Temporal**", expanded=True):
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("Sentimen Awal", f"{first_ratio:.1f}%")
            with col2:
                st.metric("Sentimen Akhir", f"{last_ratio:.1f}%")
            with col3:
                st.metric("Perubahan", f"{trend_change:+.1f}%", 
                         delta=f"{abs(trend_change):.1f}% {'naik' if trend_change > 0 else 'turun'}")
            
            if abs(trend_change) > 10:
                if trend_change > 0:
                    st.success(f"ðŸ“ˆ **Tren Positif:** Sentimen meningkat signifikan {trend_change:.1f}% dalam periode ini")
                else:
                    st.error(f"ðŸ“‰ **Tren Negatif:** Sentimen menurun signifikan {abs(trend_change):.1f}% dalam periode ini")
            elif abs(trend_change) > 5:
                if trend_change > 0:
                    st.info(f"ðŸ“Š **Tren Moderat:** Sentimen meningkat {trend_change:.1f}% dalam periode ini")
                else:
                    st.warning(f"ðŸ“Š **Tren Moderat:** Sentimen menurun {abs(trend_change):.1f}% dalam periode ini")
            else:
                st.info("ðŸ“Š **Tren Stabil:** Sentimen relatif konsisten dalam periode yang dipilih")
    
    # Key Insights Summary
    with st.expander("ðŸŽ¯ **Ringkasan Key Insights**", expanded=True):
        insights_summary = []
        
        # Insight berdasarkan volume
        if total_reviews >= 1000:
            insights_summary.append(f"ðŸ“Š **Volume Tinggi:** Analisis berdasarkan {total_reviews:,} ulasan - data sangat representative")
        elif total_reviews >= 100:
            insights_summary.append(f"ðŸ“Š **Volume Sedang:** Analisis berdasarkan {total_reviews:,} ulasan - data cukup representative")
        else:
            insights_summary.append(f"ðŸ“Š **Volume Rendah:** Hanya {total_reviews:,} ulasan - pertimbangkan periode yang lebih luas")
        
        # Insight berdasarkan distribusi
        if pos_pct >= 80:
            insights_summary.append("ðŸŽ‰ **Performa Excellent:** Mayoritas pengguna sangat puas dengan layanan GoRide")
            if pos_terms:
                top_positive = list(pos_terms.keys())[0]
                insights_summary.append(f"â­ **Kekuatan Utama:** Pengguna paling sering menyebutkan '{top_positive}' dalam ulasan positif")
        elif pos_pct >= 60:
            insights_summary.append("âœ… **Performa Baik:** Sebagian besar pengguna puas, namun masih ada ruang perbaikan")
            if neg_terms:
                top_negative = list(neg_terms.keys())[0]
                insights_summary.append(f"ðŸ” **Area Perbaikan:** Perhatikan masalah '{top_negative}' yang sering disebutkan")
        elif pos_pct >= 40:
            insights_summary.append("âš ï¸ **Performa Campuran:** Sentimen terbagi rata, perlu evaluasi menyeluruh")
            if neg_terms:
                insights_summary.append(f"ðŸš¨ **Prioritas Utama:** Segera tangani masalah '{list(neg_terms.keys())[0]}' yang dominan")
        else:
            insights_summary.append("âŒ **Performa Buruk:** Mayoritas pengguna tidak puas, diperlukan tindakan segera")
            if neg_terms:
                insights_summary.append(f"ðŸ†˜ **Krisis Alert:** Masalah kritis '{list(neg_terms.keys())[0]}' harus segera ditangani")
        
        # Tampilkan insights dengan formatting yang lebih baik
        for insight in insights_summary:
            if "Excellent" in insight or "ðŸŽ‰" in insight:
                st.success(insight)
            elif "Baik" in insight or "âœ…" in insight:
                st.success(insight)
            elif "Campuran" in insight or "âš ï¸" in insight:
                st.warning(insight)
            elif "Buruk" in insight or "âŒ" in insight or "ðŸ†˜" in insight:
                st.error(insight)
            else:
                st.info(insight)    # =============== BAGIAN REKOMENDASI YANG DIPERBAIKI ===============
    if len(neg_reviews) > 0:
        with st.expander("ðŸ”„ **Rekomendasi Tindakan & Strategi Perbaikan**", expanded=True):
            st.markdown("### ðŸ“‹ **Analisis Masalah Prioritas**")
            
            # Analisis bigram untuk masalah spesifik
            neg_text = " ".join(neg_reviews['teks_preprocessing'])
            neg_bigrams = get_ngrams(neg_text, 2, top_n=10)
            
            if neg_bigrams:
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("#### ðŸŽ¯ **Masalah Utama yang Teridentifikasi:**")
                    for i, (bigram, freq) in enumerate(list(neg_bigrams.items())[:5], 1):
                        percentage = (freq / len(neg_reviews) * 100)
                        if percentage >= 20:
                            priority = "ðŸ”´ **TINGGI**"
                        elif percentage >= 10:
                            priority = "ðŸŸ  **SEDANG**"
                        else:
                            priority = "ðŸŸ¡ **RENDAH**"
                        st.write(f"{i}. {priority} **{bigram}**")
                        st.write(f"   ðŸ“Š {freq} ulasan ({percentage:.1f}% dari ulasan negatif)")
                
                with col2:
                    st.markdown("#### ðŸ’¡ **Rekomendasi Spesifik:**")
                    
                    # Generate rekomendasi berdasarkan kata kunci yang ditemukan
                    recommendations = []
                    top_issues = list(neg_bigrams.keys())[:3]
                    
                    for issue in top_issues:
                        if any(word in issue.lower() for word in ['driver', 'sopir', 'pengemudi']):
                            recommendations.append("ðŸ‘¤ **Pelatihan Driver:** Tingkatkan kualitas layanan dan profesionalisme driver")
                        elif any(word in issue.lower() for word in ['aplikasi', 'app', 'sistem']):
                            recommendations.append("ðŸ“± **Perbaikan Aplikasi:** Optimalisasi performa dan perbaikan bug sistem")
                        elif any(word in issue.lower() for word in ['harga', 'tarif', 'biaya']):
                            recommendations.append("ðŸ’° **Review Pricing:** Evaluasi struktur tarif dan transparansi biaya")
                        elif any(word in issue.lower() for word in ['waktu', 'lama', 'lambat']):
                            recommendations.append("â±ï¸ **Efisiensi Waktu:** Optimalisasi routing dan pengurangan waktu tunggu")
                        elif any(word in issue.lower() for word in ['pelayanan', 'service', 'layanan']):
                            recommendations.append("ðŸ›Žï¸ **Service Excellence:** Peningkatan standar pelayanan customer service")
                    
                    # Tambahkan rekomendasi umum jika tidak ada yang spesifik
                    if not recommendations:
                        recommendations = [
                            "ðŸ” **Investigasi Mendalam:** Lakukan survei lanjutan untuk memahami akar masalah",
                            "ðŸ“Š **Monitoring Ketat:** Pantau metrics kualitas layanan secara real-time",
                            "ðŸŽ¯ **Focus Group:** Adakan diskusi dengan pengguna untuk feedback detail"
                        ]
                    
                    for rec in recommendations[:3]:
                        st.write(f"â€¢ {rec}")
            
            st.markdown("---")
            st.markdown("### ðŸš€ **Strategi Jangka Pendek & Panjang**")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("#### âš¡ **Aksi Segera (1-2 Minggu)**")
                urgent_actions = [
                    "ðŸ“ž Follow-up langsung dengan pengguna yang memberikan ulasan sangat negatif",
                    "ðŸ” Investigasi insiden yang paling sering disebutkan",
                    "ðŸ“¢ Komunikasi proaktif mengenai perbaikan yang sedang dilakukan",
                    "âš ï¸ Implementasi early warning system untuk mencegah masalah serupa"
                ]
                
                for action in urgent_actions:
                    st.write(f"â€¢ {action}")
            
            with col2:
                st.markdown("#### ðŸ“ˆ **Strategi Jangka Panjang (1-3 Bulan)**")
                longterm_actions = [
                    "ðŸŽ“ Program pelatihan berkelanjutan untuk driver dan customer service",
                    "ðŸ”§ Pengembangan fitur baru berdasarkan feedback pengguna",
                    "ðŸ“Š Implementasi dashboard real-time untuk monitoring kualitas layanan",
                    "ðŸ¤ Program loyalty dan reward untuk meningkatkan kepuasan pengguna"
                ]
                
                for action in longterm_actions:
                    st.write(f"â€¢ {action}")
            
            # KPI Tracking untuk Rekomendasi
            st.markdown("---")
            st.markdown("### ðŸ“Š **KPI untuk Tracking Progress**")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.markdown("**Target Mingguan:**")
                st.write("â€¢ Sentimen positif: +2-5%")
                st.write("â€¢ Response rate: >90%")
                st.write("â€¢ Resolution time: <24 jam")
            
            with col2:
                st.markdown("**Target Bulanan:**")
                st.write("â€¢ Overall satisfaction: >75%")
                st.write("â€¢ Repeat negative: <10%")
                st.write("â€¢ Driver rating: >4.5/5")
            
            with col3:
                st.markdown("**Target Kuartalan:**")
                st.write("â€¢ Market leadership: Top 2")
                st.write("â€¢ User retention: >85%")
                st.write("â€¢ NPS Score: >50")
            
            # Alert untuk status kritis
            if pos_pct < 40:
                st.error("ðŸš¨ **ALERT:** Sentimen berada dalam zona kritis! Diperlukan action plan emergency dalam 48 jam.")
            elif pos_pct < 60:
                st.warning("âš ï¸ **WARNING:** Sentimen perlu perhatian khusus. Implementasikan action plan dalam 1 minggu.")
    
    else:
        st.success("ðŸŽ‰ **Excellent Performance!** Tidak ada ulasan negatif dalam periode ini. Pertahankan kualitas layanan yang outstanding!")
        
        with st.expander("ðŸŒŸ **Strategi Mempertahankan Performa Positif**", expanded=False):
            st.write("### ðŸ’ª **Best Practices yang Harus Dipertahankan:**")
            if pos_terms:
                st.write("**Kekuatan Utama yang Diakui Pengguna:**")
                for i, (term, freq) in enumerate(list(pos_terms.items())[:5], 1):
                    st.write(f"{i}. **{term}** - disebutkan {freq} kali dalam ulasan positif")
            
            st.write("### ðŸŽ¯ **Rekomendasi untuk Sustainability:**")
            st.write("â€¢ ðŸ“Š Lakukan benchmarking rutin dengan kompetitor")
            st.write("â€¢ ðŸŽ“ Standardisasi best practices ke seluruh tim")
            st.write("â€¢ ðŸ’¬ Maintain komunikasi aktif dengan loyal customers")
            st.write("â€¢ ðŸ”„ Continuous improvement berdasarkan feedback positif")    
    # =============== FOOTER SUMMARY SECTION ===============
    st.markdown("---")
    
    # Executive Summary
    with st.expander("ðŸ“‹ **Executive Summary**", expanded=False):
        st.markdown("### ðŸŽ¯ **Ringkasan Eksekutif Dashboard**")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### ðŸ“Š **Data Overview**")
            st.write(f"â€¢ **Periode Analisis:** {start_date.strftime('%d %B %Y')} - {end_date.strftime('%d %B %Y')}")
            st.write(f"â€¢ **Total Ulasan Dianalisis:** {len(filtered_display):,}")
            st.write(f"â€¢ **Coverage Data:** {(len(filtered_display)/len(data)*100):.1f}% dari total dataset")
            st.write(f"â€¢ **Model Accuracy:** {accuracy:.1%}")
            
            st.markdown("#### ðŸŽ­ **Distribusi Sentimen**")
            st.write(f"â€¢ **Positif:** {metrics['pos_count']:,} ulasan ({metrics['pos_percentage']:.1f}%)")
            st.write(f"â€¢ **Negatif:** {metrics['neg_count']:,} ulasan ({metrics['neg_percentage']:.1f}%)")
            st.write(f"â€¢ **Indeks Kepuasan:** {metrics['pos_pct']:.1f}%")
        
        with col2:
            st.markdown("#### ðŸ” **Key Findings**")
            if metrics['pos_percentage'] >= 70:
                st.success("âœ… **Status:** Performa layanan excellent")
                st.write("â€¢ Mayoritas pengguna puas dengan layanan")
                st.write("â€¢ Pertahankan kualitas dan konsistensi")
            elif metrics['pos_percentage'] >= 50:
                st.info("â„¹ï¸ **Status:** Performa layanan baik dengan ruang perbaikan")
                st.write("â€¢ Performa di atas rata-rata industri")
                st.write("â€¢ Focus pada peningkatan area negatif")
            else:
                st.warning("âš ï¸ **Status:** Performa layanan perlu perhatian serius")
                st.write("â€¢ Diperlukan action plan immediate")
                st.write("â€¢ Review fundamental layanan")
            
            if selected_topic != "All":
                st.markdown("#### ðŸ·ï¸ **Filter Aktif**")
                st.write(f"â€¢ **Topik:** {selected_topic}")
                st.write(f"â€¢ **Data Filtered:** {len(topic_data):,} ulasan")
    
    # Technical Info
    with st.expander("âš™ï¸ **Informasi Teknis**", expanded=False):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("**Model Information:**")
            st.write(f"â€¢ Algorithm: SVM")
            st.write(f"â€¢ Vectorizer: TF-IDF")
            st.write(f"â€¢ Accuracy: {accuracy:.3f}")
            st.write(f"â€¢ Precision: {precision:.3f}")
            st.write(f"â€¢ Recall: {recall:.3f}")
            st.write(f"â€¢ F1-Score: {f1:.3f}")
        
        with col2:
            st.markdown("**Data Processing:**")
            st.write("â€¢ Preprocessing: âœ… Aktif")
            st.write("â€¢ Text Cleaning: âœ… Aktif")
            st.write("â€¢ Stopword Removal: âœ… Aktif")
            st.write("â€¢ Stemming: âœ… Aktif")
            st.write("â€¢ Normalization: âœ… Aktif")
        
        with col3:
            st.markdown("**System Status:**")
            st.write("â€¢ Data Loading: âœ… Success")
            st.write("â€¢ Model Loading: âœ… Success")
            st.write("â€¢ Cache Status: âœ… Active")
            st.write("â€¢ Performance: âœ… Optimal")
    
    st.markdown("---")
    
    # Footer
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown("""
        <div style='text-align: center'>
            <h4>ðŸ“Š GoRide Sentiment Analysis Dashboard</h4>
            <p><b>Â© 2025 GoRide Sentiment Analysis App</b></p>
            <p>Developed by <b>Mhd Adreansyah</b></p>
            <p><i>Aplikasi ini merupakan Tugas Akhir/Skripsi dibawah perlindungan Hak Cipta</i></p>
            <hr>
            <p style='font-size: 12px; color: #666;'>
                ðŸ”’ Confidential & Proprietary | ðŸ›¡ï¸ Data Protected | âš¡ Real-time Analytics
            </p>
        </div>
        """, unsafe_allow_html=True)